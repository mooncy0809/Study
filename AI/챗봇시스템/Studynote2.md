[01] 수치화, 원-핫 인코딩(One-hot encoding), 분산 표현 방법(밀집 표현 방법, Word2Vec)
   - 컴퓨터는 자연어를 숫자나 벡터로 변경해야 처리 가능함으로 자연어를 수치화 시키는 방법

1. 정수화
   - 일반적으로 단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수를 내림차순으로 1부터 정수 부여
   - Python: 1, DBMS: 2, JAVA: 3: Python, JAVA: 객체지향 이론이 거의 비슷함으로 전혀 다른 언어는 아님, 단어들간의 '비슷한 정도 표현이 어려움'.


2. 원-핫 인코딩(One-hot encoding)
   - '원-핫 인코딩'으로 나온 값을 '원-핫 벡터'라고하며, 전체 요소중 단 하나의 값만 1임으로 '희소 벡터(희소 행렬)'하고함.
   - 각각의 분류에 속할 확률이 출력됨.
   - 단어가 100개이면 벡터의 크기는 100 차원이됨, 출력도 100가지가됨.
   - 다중 분류, 단어가 많아지면 메모리 낭비가 심하게 발생함.<Br>![image](https://user-images.githubusercontent.com/84116509/177171810-4c7e89cd-dd26-4d42-a86b-eda24299917a.png)

  
[02] 분산표현과 희소 표현
1) 희소 표현(One-hot encoding)
   - 분류 종류가 적은 경우 사용 권장
   - 분류 종류가 많다면 많은 차원을 사용해야하며 분류되는 값만 1을 갖으며 나머지는 0의 값을 갖는 비효율적인구조임.
   - 남자와 남성은 전혀 다른 단어로 분류됨, 의미 표현이 부족함.
   - 확률이 가장 높은 분류값을 찾아서 결정함, argmax() 사용.<br>![image](https://user-images.githubusercontent.com/84116509/177171908-5871e54f-52b3-40ec-95aa-2df8a6f38377.png)
2) 분산 표현 방법(밀집 표현 방법, Word2Vec)
   - 적은 차원으로 대규모의 분류 표현 가능<br>![image](https://user-images.githubusercontent.com/84116509/177171939-43872af1-2e9c-4db5-a6d8-2418ec7b4857.png)
   - 벡터(수치화) 공간에서 비슷한 의미를 갖는 단어들은 비슷한 위치에 분포됨으로 남자와 남성의 단어 위치는 매우 가까움.
     이런 두 단어간의 거리를 계산 할 수 있으면 컴퓨터는 '남자'와 '남성' 두 단어를 같은 의미로 해석 할 수 있어 유용한 기법임. <br>![image](https://user-images.githubusercontent.com/84116509/177171989-772e8c8c-3bfd-4a62-aaa9-5ee8f4011bd2.png)

3. Word2Vec
   - 분포 가설(Distributed hypothesis)에 기반함
   - 분포 가설은 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다 라는 의미이다.
     따라서 어떤 글의 비슷한 위치에 존재하는 단어는 단어 간의 유사도를 높게 측정함.
   - 1에 가까울수록 유사도가 높음, 좋은 말뭉치(문장들의 모임) 모델을 사용시 언어 학습이 완성도있게 이루어짐.
   - 신경망 기반의 대표적인 '워드 임베딩' package ★
   - 2013년 Google에서 발표했으며 가장 많이 사용되는 워드 임베딩 방법
   - 기존의 신경망 모델과 비슷한 구조를 가지고 있으나 계산량을 획기적으로 줄여 빠른 학습을 가능하게함.
   - 사용법이 간단하며 성능이 나쁘지않은 Gensim 패키지를 많이 사용함.
   - Word2Vec을 만들기위해서는 한국어 말뭉치(단어의 집합)를 이용해야함.

1) CBOW 모델
   - 맥락이라 표현되는 주변 단어들을 이용해 타깃 단어를 예측하는 신경망 모델
   - 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용함.


2) skip-gram 모델
   - CBOW 모델과 반대로 하나의 타깃 단어를 이용해 주변 단어들을 예측하는 신경망 모델
   - CBOW 모델에 비해 예측해야 하는 맥락이 많아짐으로 단어 분산 표현력이 우수해 CBOW 모델에 비해 품질이 우수함.
   - CBOW 모델은 타깃 단어의 손실만 계산하면 됨으로 학습 속도가 빠른 장점이 있음.

3) 모델 비교<br>![image](https://user-images.githubusercontent.com/84116509/177172037-eba92116-5183-4441-b0a2-a474bf89e318.png)
4) 윈도우 크기: 앞뒤로 몇개의 단어를 확인할 것인지의 크기<br>![image](https://user-images.githubusercontent.com/84116509/177172097-78e6fbfd-958c-4d6e-a9c8-6f05b615e8d5.png)
5) Word2Vec은 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치 시킴 <br>![image](https://user-images.githubusercontent.com/84116509/177172150-0c5aeac4-b6e8-4d1f-9122-f48ac8f7137a.png)

