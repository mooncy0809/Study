{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sin곡선을 예측하는 순환신경망 LSTM 모델의 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.layers import Flatten, LSTM, SimpleRNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.utils import plot_model   # 네트워크 입출력 시각화\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "rc('font', family='Malgun Gothic')\n",
    "plt.rcParams[\"font.size\"] = 12         # 글자 크기\n",
    "# plt.rcParams[\"figure.figsize\"] = (10, 4) # 10:4의 그래프 비율\n",
    "plt.rcParams['axes.unicode_minus'] = False  # minus 부호는 unicode 적용시 한글이 깨짐으로 설정\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep 만큼 시퀀스 데이터 분리\n",
    "def split_sequence(sequence, step):\n",
    "    x, y = list(), list()\n",
    "\n",
    "    for i in range(len(sequence)):\n",
    "        end_idx = i + step\n",
    "        if end_idx > len(sequence) - 1:\n",
    "            break\n",
    "            \n",
    "        # seq_x: 0 ~ 14, seq_y: 15\n",
    "        # seq_x: 1~ 15, seq_y: 16\n",
    "        # seq_x: 2~ 16, seq_y: 17\n",
    "        seq_x, seq_y = sequence[i:end_idx], sequence[end_idx]\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin 함수 학습 데이터\n",
    "x = [i for i in np.arange(start=-10, stop=10, step=0.1)]\n",
    "train_y = [np.sin(i) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape x:(185, 15) / y:(185,)\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "n_timesteps = 15 # 입력 시퀀스 길이, 메모리셀은 15개가됨.\n",
    "n_features = 1   # 변수의 갯수 1개, 특성 벡터의 갯수\n",
    "\n",
    "# 시퀀스 나누기\n",
    "# train_x.shape => (samples, timesteps)\n",
    "# train_y.shape => (samples)\n",
    "train_x, train_y = split_sequence(train_y, step=n_timesteps)\n",
    "print(\"shape x:{} / y:{}\".format(train_x.shape, train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape = (185, 15, 1)\n",
      "train_y.shape = (185,)\n"
     ]
    }
   ],
   "source": [
    "# RNN 입력 벡터 크기를 맞추기 위해 벡터 차원 크기 변경\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], n_features)\n",
    "print(\"train_x.shape = {}\".format(train_x.shape))\n",
    "print(\"train_y.shape = {}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/5 [==============================] - 3s 300ms/step - loss: 0.8013 - val_loss: 0.7204\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.77492, saving model to .\\LSTM.h5\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6769 - val_loss: 0.6631\n",
      "\n",
      "Epoch 00002: loss improved from 0.77492 to 0.71056, saving model to .\\LSTM.h5\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6701 - val_loss: 0.6106\n",
      "\n",
      "Epoch 00003: loss improved from 0.71056 to 0.65554, saving model to .\\LSTM.h5\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.5798 - val_loss: 0.5645\n",
      "\n",
      "Epoch 00004: loss improved from 0.65554 to 0.60408, saving model to .\\LSTM.h5\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5725 - val_loss: 0.5212\n",
      "\n",
      "Epoch 00005: loss improved from 0.60408 to 0.55777, saving model to .\\LSTM.h5\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4982 - val_loss: 0.4803\n",
      "\n",
      "Epoch 00006: loss improved from 0.55777 to 0.51374, saving model to .\\LSTM.h5\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4735 - val_loss: 0.4401\n",
      "\n",
      "Epoch 00007: loss improved from 0.51374 to 0.47278, saving model to .\\LSTM.h5\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4358 - val_loss: 0.4039\n",
      "\n",
      "Epoch 00008: loss improved from 0.47278 to 0.43311, saving model to .\\LSTM.h5\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4064 - val_loss: 0.3742\n",
      "\n",
      "Epoch 00009: loss improved from 0.43311 to 0.39560, saving model to .\\LSTM.h5\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3504 - val_loss: 0.3473\n",
      "\n",
      "Epoch 00010: loss improved from 0.39560 to 0.36538, saving model to .\\LSTM.h5\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3308 - val_loss: 0.3225\n",
      "\n",
      "Epoch 00011: loss improved from 0.36538 to 0.33820, saving model to .\\LSTM.h5\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.3207 - val_loss: 0.2990\n",
      "\n",
      "Epoch 00012: loss improved from 0.33820 to 0.31371, saving model to .\\LSTM.h5\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2921 - val_loss: 0.2778\n",
      "\n",
      "Epoch 00013: loss improved from 0.31371 to 0.29088, saving model to .\\LSTM.h5\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2676 - val_loss: 0.2617\n",
      "\n",
      "Epoch 00014: loss improved from 0.29088 to 0.27230, saving model to .\\LSTM.h5\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2572 - val_loss: 0.2489\n",
      "\n",
      "Epoch 00015: loss improved from 0.27230 to 0.25493, saving model to .\\LSTM.h5\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.2320 - val_loss: 0.2349\n",
      "\n",
      "Epoch 00016: loss improved from 0.25493 to 0.24147, saving model to .\\LSTM.h5\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2302 - val_loss: 0.2198\n",
      "\n",
      "Epoch 00017: loss improved from 0.24147 to 0.22810, saving model to .\\LSTM.h5\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.2226 - val_loss: 0.2053\n",
      "\n",
      "Epoch 00018: loss improved from 0.22810 to 0.21489, saving model to .\\LSTM.h5\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2075 - val_loss: 0.1934\n",
      "\n",
      "Epoch 00019: loss improved from 0.21489 to 0.20292, saving model to .\\LSTM.h5\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1939 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00020: loss improved from 0.20292 to 0.19263, saving model to .\\LSTM.h5\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1841 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00021: loss improved from 0.19263 to 0.18298, saving model to .\\LSTM.h5\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1700 - val_loss: 0.1642\n",
      "\n",
      "Epoch 00022: loss improved from 0.18298 to 0.17367, saving model to .\\LSTM.h5\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1715 - val_loss: 0.1562\n",
      "\n",
      "Epoch 00023: loss improved from 0.17367 to 0.16569, saving model to .\\LSTM.h5\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.1563 - val_loss: 0.1487\n",
      "\n",
      "Epoch 00024: loss improved from 0.16569 to 0.15830, saving model to .\\LSTM.h5\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1590 - val_loss: 0.1407\n",
      "\n",
      "Epoch 00025: loss improved from 0.15830 to 0.15123, saving model to .\\LSTM.h5\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1441 - val_loss: 0.1340\n",
      "\n",
      "Epoch 00026: loss improved from 0.15123 to 0.14397, saving model to .\\LSTM.h5\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.1381 - val_loss: 0.1265\n",
      "\n",
      "Epoch 00027: loss improved from 0.14397 to 0.13740, saving model to .\\LSTM.h5\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1391 - val_loss: 0.1192\n",
      "\n",
      "Epoch 00028: loss improved from 0.13740 to 0.13127, saving model to .\\LSTM.h5\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1283 - val_loss: 0.1130\n",
      "\n",
      "Epoch 00029: loss improved from 0.13127 to 0.12529, saving model to .\\LSTM.h5\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.1202 - val_loss: 0.1085\n",
      "\n",
      "Epoch 00030: loss improved from 0.12529 to 0.12003, saving model to .\\LSTM.h5\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1152 - val_loss: 0.1048\n",
      "\n",
      "Epoch 00031: loss improved from 0.12003 to 0.11520, saving model to .\\LSTM.h5\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1120 - val_loss: 0.1014\n",
      "\n",
      "Epoch 00032: loss improved from 0.11520 to 0.11013, saving model to .\\LSTM.h5\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1055 - val_loss: 0.0983\n",
      "\n",
      "Epoch 00033: loss improved from 0.11013 to 0.10527, saving model to .\\LSTM.h5\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.1030 - val_loss: 0.0958\n",
      "\n",
      "Epoch 00034: loss improved from 0.10527 to 0.10051, saving model to .\\LSTM.h5\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1003 - val_loss: 0.0933\n",
      "\n",
      "Epoch 00035: loss improved from 0.10051 to 0.09671, saving model to .\\LSTM.h5\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0919 - val_loss: 0.0884\n",
      "\n",
      "Epoch 00036: loss improved from 0.09671 to 0.09192, saving model to .\\LSTM.h5\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0857 - val_loss: 0.0842\n",
      "\n",
      "Epoch 00037: loss improved from 0.09192 to 0.08753, saving model to .\\LSTM.h5\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0793 - val_loss: 0.0798\n",
      "\n",
      "Epoch 00038: loss improved from 0.08753 to 0.08315, saving model to .\\LSTM.h5\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0796 - val_loss: 0.0752\n",
      "\n",
      "Epoch 00039: loss improved from 0.08315 to 0.07902, saving model to .\\LSTM.h5\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0752 - val_loss: 0.0708\n",
      "\n",
      "Epoch 00040: loss improved from 0.07902 to 0.07500, saving model to .\\LSTM.h5\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0746 - val_loss: 0.0676\n",
      "\n",
      "Epoch 00041: loss improved from 0.07500 to 0.07109, saving model to .\\LSTM.h5\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0701 - val_loss: 0.0649\n",
      "\n",
      "Epoch 00042: loss improved from 0.07109 to 0.06734, saving model to .\\LSTM.h5\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0641 - val_loss: 0.0626\n",
      "\n",
      "Epoch 00043: loss improved from 0.06734 to 0.06347, saving model to .\\LSTM.h5\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0611 - val_loss: 0.0606\n",
      "\n",
      "Epoch 00044: loss improved from 0.06347 to 0.06032, saving model to .\\LSTM.h5\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0589 - val_loss: 0.0584\n",
      "\n",
      "Epoch 00045: loss improved from 0.06032 to 0.05740, saving model to .\\LSTM.h5\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0531 - val_loss: 0.0549\n",
      "\n",
      "Epoch 00046: loss improved from 0.05740 to 0.05415, saving model to .\\LSTM.h5\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0521 - val_loss: 0.0511\n",
      "\n",
      "Epoch 00047: loss improved from 0.05415 to 0.05080, saving model to .\\LSTM.h5\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0447 - val_loss: 0.0476\n",
      "\n",
      "Epoch 00048: loss improved from 0.05080 to 0.04728, saving model to .\\LSTM.h5\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0447 - val_loss: 0.0445\n",
      "\n",
      "Epoch 00049: loss improved from 0.04728 to 0.04435, saving model to .\\LSTM.h5\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0438 - val_loss: 0.0419\n",
      "\n",
      "Epoch 00050: loss improved from 0.04435 to 0.04188, saving model to .\\LSTM.h5\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0392 - val_loss: 0.0396\n",
      "\n",
      "Epoch 00051: loss improved from 0.04188 to 0.03932, saving model to .\\LSTM.h5\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0392 - val_loss: 0.0372\n",
      "\n",
      "Epoch 00052: loss improved from 0.03932 to 0.03689, saving model to .\\LSTM.h5\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0347 - val_loss: 0.0350\n",
      "\n",
      "Epoch 00053: loss improved from 0.03689 to 0.03443, saving model to .\\LSTM.h5\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0318 - val_loss: 0.0331\n",
      "\n",
      "Epoch 00054: loss improved from 0.03443 to 0.03229, saving model to .\\LSTM.h5\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0314 - val_loss: 0.0312\n",
      "\n",
      "Epoch 00055: loss improved from 0.03229 to 0.03024, saving model to .\\LSTM.h5\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0297 - val_loss: 0.0293\n",
      "\n",
      "Epoch 00056: loss improved from 0.03024 to 0.02824, saving model to .\\LSTM.h5\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0272 - val_loss: 0.0275\n",
      "\n",
      "Epoch 00057: loss improved from 0.02824 to 0.02624, saving model to .\\LSTM.h5\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0263 - val_loss: 0.0256\n",
      "\n",
      "Epoch 00058: loss improved from 0.02624 to 0.02451, saving model to .\\LSTM.h5\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0233 - val_loss: 0.0236\n",
      "\n",
      "Epoch 00059: loss improved from 0.02451 to 0.02253, saving model to .\\LSTM.h5\n",
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0220 - val_loss: 0.0216\n",
      "\n",
      "Epoch 00060: loss improved from 0.02253 to 0.02028, saving model to .\\LSTM.h5\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0190 - val_loss: 0.0202\n",
      "\n",
      "Epoch 00061: loss improved from 0.02028 to 0.01844, saving model to .\\LSTM.h5\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0186 - val_loss: 0.0191\n",
      "\n",
      "Epoch 00062: loss improved from 0.01844 to 0.01749, saving model to .\\LSTM.h5\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0154 - val_loss: 0.0180\n",
      "\n",
      "Epoch 00063: loss improved from 0.01749 to 0.01646, saving model to .\\LSTM.h5\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0152 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00064: loss improved from 0.01646 to 0.01523, saving model to .\\LSTM.h5\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0140 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00065: loss improved from 0.01523 to 0.01402, saving model to .\\LSTM.h5\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0138 - val_loss: 0.0142\n",
      "\n",
      "Epoch 00066: loss improved from 0.01402 to 0.01314, saving model to .\\LSTM.h5\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0120 - val_loss: 0.0129\n",
      "\n",
      "Epoch 00067: loss improved from 0.01314 to 0.01208, saving model to .\\LSTM.h5\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0107 - val_loss: 0.0120\n",
      "\n",
      "Epoch 00068: loss improved from 0.01208 to 0.01106, saving model to .\\LSTM.h5\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0100 - val_loss: 0.0111\n",
      "\n",
      "Epoch 00069: loss improved from 0.01106 to 0.01027, saving model to .\\LSTM.h5\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0093 - val_loss: 0.0104\n",
      "\n",
      "Epoch 00070: loss improved from 0.01027 to 0.00963, saving model to .\\LSTM.h5\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0092 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00071: loss improved from 0.00963 to 0.00903, saving model to .\\LSTM.h5\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0083 - val_loss: 0.0091\n",
      "\n",
      "Epoch 00072: loss improved from 0.00903 to 0.00846, saving model to .\\LSTM.h5\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0082 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00073: loss improved from 0.00846 to 0.00812, saving model to .\\LSTM.h5\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0076 - val_loss: 0.0081\n",
      "\n",
      "Epoch 00074: loss improved from 0.00812 to 0.00769, saving model to .\\LSTM.h5\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0085 - val_loss: 0.0077\n",
      "\n",
      "Epoch 00075: loss improved from 0.00769 to 0.00729, saving model to .\\LSTM.h5\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0069 - val_loss: 0.0074\n",
      "\n",
      "Epoch 00076: loss improved from 0.00729 to 0.00690, saving model to .\\LSTM.h5\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0064 - val_loss: 0.0070\n",
      "\n",
      "Epoch 00077: loss improved from 0.00690 to 0.00660, saving model to .\\LSTM.h5\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0058 - val_loss: 0.0067\n",
      "\n",
      "Epoch 00078: loss improved from 0.00660 to 0.00632, saving model to .\\LSTM.h5\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0057 - val_loss: 0.0063\n",
      "\n",
      "Epoch 00079: loss improved from 0.00632 to 0.00595, saving model to .\\LSTM.h5\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0059 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00080: loss improved from 0.00595 to 0.00565, saving model to .\\LSTM.h5\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0049 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00081: loss improved from 0.00565 to 0.00543, saving model to .\\LSTM.h5\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0053 - val_loss: 0.0056\n",
      "\n",
      "Epoch 00082: loss improved from 0.00543 to 0.00531, saving model to .\\LSTM.h5\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0051 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00083: loss improved from 0.00531 to 0.00515, saving model to .\\LSTM.h5\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0052 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00084: loss improved from 0.00515 to 0.00488, saving model to .\\LSTM.h5\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "\n",
      "Epoch 00085: loss improved from 0.00488 to 0.00460, saving model to .\\LSTM.h5\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "\n",
      "Epoch 00086: loss improved from 0.00460 to 0.00432, saving model to .\\LSTM.h5\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0044 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00087: loss improved from 0.00432 to 0.00412, saving model to .\\LSTM.h5\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "\n",
      "Epoch 00088: loss improved from 0.00412 to 0.00396, saving model to .\\LSTM.h5\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0039 - val_loss: 0.0040\n",
      "\n",
      "Epoch 00089: loss improved from 0.00396 to 0.00378, saving model to .\\LSTM.h5\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0036 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00090: loss improved from 0.00378 to 0.00361, saving model to .\\LSTM.h5\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0032 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00091: loss improved from 0.00361 to 0.00347, saving model to .\\LSTM.h5\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00092: loss improved from 0.00347 to 0.00338, saving model to .\\LSTM.h5\n",
      "Epoch 93/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00093: loss improved from 0.00338 to 0.00334, saving model to .\\LSTM.h5\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0031 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00094: loss improved from 0.00334 to 0.00321, saving model to .\\LSTM.h5\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0030 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00095: loss improved from 0.00321 to 0.00315, saving model to .\\LSTM.h5\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0030 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00096: loss improved from 0.00315 to 0.00303, saving model to .\\LSTM.h5\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0028 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00097: loss improved from 0.00303 to 0.00285, saving model to .\\LSTM.h5\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00098: loss improved from 0.00285 to 0.00272, saving model to .\\LSTM.h5\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0027 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00099: loss improved from 0.00272 to 0.00256, saving model to .\\LSTM.h5\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0024 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00100: loss improved from 0.00256 to 0.00247, saving model to .\\LSTM.h5\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.00247\n",
      "Epoch 102/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.00247\n",
      "Epoch 103/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0027 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00103: loss did not improve from 0.00247\n",
      "Epoch 104/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0025 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00104: loss improved from 0.00247 to 0.00245, saving model to .\\LSTM.h5\n",
      "Epoch 105/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00105: loss improved from 0.00245 to 0.00223, saving model to .\\LSTM.h5\n",
      "Epoch 106/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0022 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00106: loss improved from 0.00223 to 0.00208, saving model to .\\LSTM.h5\n",
      "Epoch 107/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00107: loss improved from 0.00208 to 0.00207, saving model to .\\LSTM.h5\n",
      "Epoch 108/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00108: loss improved from 0.00207 to 0.00202, saving model to .\\LSTM.h5\n",
      "Epoch 109/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00109: loss improved from 0.00202 to 0.00192, saving model to .\\LSTM.h5\n",
      "Epoch 110/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0020 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.00192\n",
      "Epoch 111/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.00192\n",
      "Epoch 112/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.00192\n",
      "Epoch 113/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00113: loss improved from 0.00192 to 0.00182, saving model to .\\LSTM.h5\n",
      "Epoch 114/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00114: loss improved from 0.00182 to 0.00176, saving model to .\\LSTM.h5\n",
      "Epoch 115/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00115: loss did not improve from 0.00176\n",
      "Epoch 116/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.00176\n",
      "Epoch 117/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00117: loss improved from 0.00176 to 0.00171, saving model to .\\LSTM.h5\n",
      "Epoch 118/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00118: loss improved from 0.00171 to 0.00163, saving model to .\\LSTM.h5\n",
      "Epoch 119/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00119: loss improved from 0.00163 to 0.00160, saving model to .\\LSTM.h5\n",
      "Epoch 120/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00120: loss improved from 0.00160 to 0.00158, saving model to .\\LSTM.h5\n",
      "Epoch 121/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00121: loss improved from 0.00158 to 0.00156, saving model to .\\LSTM.h5\n",
      "Epoch 122/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00122: loss improved from 0.00156 to 0.00150, saving model to .\\LSTM.h5\n",
      "Epoch 123/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.00150\n",
      "Epoch 124/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0015 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.00150\n",
      "Epoch 125/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.00150\n",
      "Epoch 126/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00126: loss improved from 0.00150 to 0.00146, saving model to .\\LSTM.h5\n",
      "Epoch 127/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00127: loss improved from 0.00146 to 0.00141, saving model to .\\LSTM.h5\n",
      "Epoch 128/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00128: loss did not improve from 0.00141\n",
      "Epoch 129/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00129: loss did not improve from 0.00141\n",
      "Epoch 130/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00130: loss did not improve from 0.00141\n",
      "Epoch 131/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.00141\n",
      "Epoch 132/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00132: loss improved from 0.00141 to 0.00129, saving model to .\\LSTM.h5\n",
      "Epoch 133/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00133: loss improved from 0.00129 to 0.00128, saving model to .\\LSTM.h5\n",
      "Epoch 134/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00134: loss did not improve from 0.00128\n",
      "Epoch 135/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00135: loss did not improve from 0.00128\n",
      "Epoch 136/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00136: loss improved from 0.00128 to 0.00126, saving model to .\\LSTM.h5\n",
      "Epoch 137/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00137: loss improved from 0.00126 to 0.00121, saving model to .\\LSTM.h5\n",
      "Epoch 138/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00138: loss improved from 0.00121 to 0.00119, saving model to .\\LSTM.h5\n",
      "Epoch 139/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00139: loss improved from 0.00119 to 0.00116, saving model to .\\LSTM.h5\n",
      "Epoch 140/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00140: loss did not improve from 0.00116\n",
      "Epoch 141/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00141: loss improved from 0.00116 to 0.00116, saving model to .\\LSTM.h5\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00142: loss improved from 0.00116 to 0.00113, saving model to .\\LSTM.h5\n",
      "Epoch 143/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00143: loss improved from 0.00113 to 0.00107, saving model to .\\LSTM.h5\n",
      "Epoch 144/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00144: loss improved from 0.00107 to 0.00107, saving model to .\\LSTM.h5\n",
      "Epoch 145/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00145: loss did not improve from 0.00107\n",
      "Epoch 146/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00146: loss did not improve from 0.00107\n",
      "Epoch 147/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00147: loss did not improve from 0.00107\n",
      "Epoch 148/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00148: loss improved from 0.00107 to 0.00101, saving model to .\\LSTM.h5\n",
      "Epoch 149/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00149: loss improved from 0.00101 to 0.00099, saving model to .\\LSTM.h5\n",
      "Epoch 150/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.5720e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00150: loss improved from 0.00099 to 0.00096, saving model to .\\LSTM.h5\n",
      "Epoch 151/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.4710e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00151: loss improved from 0.00096 to 0.00094, saving model to .\\LSTM.h5\n",
      "Epoch 152/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.2072e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00152: loss improved from 0.00094 to 0.00094, saving model to .\\LSTM.h5\n",
      "Epoch 153/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.1499e-04 - val_loss: 9.7937e-04\n",
      "\n",
      "Epoch 00153: loss improved from 0.00094 to 0.00091, saving model to .\\LSTM.h5\n",
      "Epoch 154/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.7491e-04 - val_loss: 9.5435e-04\n",
      "\n",
      "Epoch 00154: loss improved from 0.00091 to 0.00090, saving model to .\\LSTM.h5\n",
      "Epoch 155/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.5584e-04 - val_loss: 9.3630e-04\n",
      "\n",
      "Epoch 00155: loss improved from 0.00090 to 0.00088, saving model to .\\LSTM.h5\n",
      "Epoch 156/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.6306e-04 - val_loss: 9.3822e-04\n",
      "\n",
      "Epoch 00156: loss improved from 0.00088 to 0.00087, saving model to .\\LSTM.h5\n",
      "Epoch 157/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 8.1258e-04 - val_loss: 9.1362e-04\n",
      "\n",
      "Epoch 00157: loss improved from 0.00087 to 0.00085, saving model to .\\LSTM.h5\n",
      "Epoch 158/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.4739e-04 - val_loss: 9.2492e-04\n",
      "\n",
      "Epoch 00158: loss improved from 0.00085 to 0.00084, saving model to .\\LSTM.h5\n",
      "Epoch 159/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.7254e-04 - val_loss: 9.3986e-04\n",
      "\n",
      "Epoch 00159: loss did not improve from 0.00084\n",
      "Epoch 160/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 9.3598e-04 - val_loss: 9.2626e-04\n",
      "\n",
      "Epoch 00160: loss did not improve from 0.00084\n",
      "Epoch 161/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.7944e-04 - val_loss: 8.9607e-04\n",
      "\n",
      "Epoch 00161: loss improved from 0.00084 to 0.00084, saving model to .\\LSTM.h5\n",
      "Epoch 162/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.7559e-04 - val_loss: 9.7412e-04\n",
      "\n",
      "Epoch 00162: loss improved from 0.00084 to 0.00081, saving model to .\\LSTM.h5\n",
      "Epoch 163/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.6173e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.00081\n",
      "Epoch 164/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.9917e-04 - val_loss: 8.1988e-04\n",
      "\n",
      "Epoch 00164: loss did not improve from 0.00081\n",
      "Epoch 165/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.1444e-04 - val_loss: 9.4619e-04\n",
      "\n",
      "Epoch 00165: loss improved from 0.00081 to 0.00076, saving model to .\\LSTM.h5\n",
      "Epoch 166/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.5465e-04 - val_loss: 9.0576e-04\n",
      "\n",
      "Epoch 00166: loss did not improve from 0.00076\n",
      "Epoch 167/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 8.2600e-04 - val_loss: 8.1116e-04\n",
      "\n",
      "Epoch 00167: loss did not improve from 0.00076\n",
      "Epoch 168/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2721e-04 - val_loss: 7.6841e-04\n",
      "\n",
      "Epoch 00168: loss improved from 0.00076 to 0.00072, saving model to .\\LSTM.h5\n",
      "Epoch 169/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 6.8797e-04 - val_loss: 7.8385e-04\n",
      "\n",
      "Epoch 00169: loss improved from 0.00072 to 0.00069, saving model to .\\LSTM.h5\n",
      "Epoch 170/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.8606e-04 - val_loss: 7.9792e-04\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.00069\n",
      "Epoch 171/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.1131e-04 - val_loss: 7.2956e-04\n",
      "\n",
      "Epoch 00171: loss improved from 0.00069 to 0.00066, saving model to .\\LSTM.h5\n",
      "Epoch 172/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 6.4451e-04 - val_loss: 8.3591e-04\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.00066\n",
      "Epoch 173/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.7256e-04 - val_loss: 8.1411e-04\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.00066\n",
      "Epoch 174/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.6193e-04 - val_loss: 7.5007e-04\n",
      "\n",
      "Epoch 00174: loss did not improve from 0.00066\n",
      "Epoch 175/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.0879e-04 - val_loss: 7.1743e-04\n",
      "\n",
      "Epoch 00175: loss improved from 0.00066 to 0.00066, saving model to .\\LSTM.h5\n",
      "Epoch 176/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7383e-04 - val_loss: 7.2033e-04\n",
      "\n",
      "Epoch 00176: loss improved from 0.00066 to 0.00065, saving model to .\\LSTM.h5\n",
      "Epoch 177/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7229e-04 - val_loss: 7.1849e-04\n",
      "\n",
      "Epoch 00177: loss did not improve from 0.00065\n",
      "Epoch 178/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.8299e-04 - val_loss: 6.5727e-04\n",
      "\n",
      "Epoch 00178: loss improved from 0.00065 to 0.00064, saving model to .\\LSTM.h5\n",
      "Epoch 179/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.5490e-04 - val_loss: 6.2921e-04\n",
      "\n",
      "Epoch 00179: loss improved from 0.00064 to 0.00058, saving model to .\\LSTM.h5\n",
      "Epoch 180/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.3442e-04 - val_loss: 6.1671e-04\n",
      "\n",
      "Epoch 00180: loss improved from 0.00058 to 0.00057, saving model to .\\LSTM.h5\n",
      "Epoch 181/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.2244e-04 - val_loss: 5.9707e-04\n",
      "\n",
      "Epoch 00181: loss improved from 0.00057 to 0.00054, saving model to .\\LSTM.h5\n",
      "Epoch 182/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4261e-04 - val_loss: 5.8807e-04\n",
      "\n",
      "Epoch 00182: loss improved from 0.00054 to 0.00053, saving model to .\\LSTM.h5\n",
      "Epoch 183/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.5020e-04 - val_loss: 5.7570e-04\n",
      "\n",
      "Epoch 00183: loss improved from 0.00053 to 0.00052, saving model to .\\LSTM.h5\n",
      "Epoch 184/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 4.9416e-04 - val_loss: 5.6980e-04\n",
      "\n",
      "Epoch 00184: loss improved from 0.00052 to 0.00052, saving model to .\\LSTM.h5\n",
      "Epoch 185/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.0364e-04 - val_loss: 5.6879e-04\n",
      "\n",
      "Epoch 00185: loss improved from 0.00052 to 0.00051, saving model to .\\LSTM.h5\n",
      "Epoch 186/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 5.0685e-04 - val_loss: 5.7370e-04\n",
      "\n",
      "Epoch 00186: loss improved from 0.00051 to 0.00051, saving model to .\\LSTM.h5\n",
      "Epoch 187/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1441e-04 - val_loss: 5.8162e-04\n",
      "\n",
      "Epoch 00187: loss did not improve from 0.00051\n",
      "Epoch 188/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.1004e-04 - val_loss: 5.9213e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00188: loss did not improve from 0.00051\n",
      "Epoch 189/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 5.1331e-04 - val_loss: 6.2592e-04\n",
      "\n",
      "Epoch 00189: loss did not improve from 0.00051\n",
      "Epoch 190/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.4940e-04 - val_loss: 5.8155e-04\n",
      "\n",
      "Epoch 00190: loss did not improve from 0.00051\n",
      "Epoch 191/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 4.8328e-04 - val_loss: 5.1389e-04\n",
      "\n",
      "Epoch 00191: loss improved from 0.00051 to 0.00049, saving model to .\\LSTM.h5\n",
      "Epoch 192/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.7114e-04 - val_loss: 4.9094e-04\n",
      "\n",
      "Epoch 00192: loss improved from 0.00049 to 0.00045, saving model to .\\LSTM.h5\n",
      "Epoch 193/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.2332e-04 - val_loss: 4.7752e-04\n",
      "\n",
      "Epoch 00193: loss improved from 0.00045 to 0.00043, saving model to .\\LSTM.h5\n",
      "Epoch 194/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.8892e-04 - val_loss: 4.7131e-04\n",
      "\n",
      "Epoch 00194: loss improved from 0.00043 to 0.00042, saving model to .\\LSTM.h5\n",
      "Epoch 195/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.2219e-04 - val_loss: 4.6330e-04\n",
      "\n",
      "Epoch 00195: loss improved from 0.00042 to 0.00042, saving model to .\\LSTM.h5\n",
      "Epoch 196/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.4160e-04 - val_loss: 4.6259e-04\n",
      "\n",
      "Epoch 00196: loss improved from 0.00042 to 0.00041, saving model to .\\LSTM.h5\n",
      "Epoch 197/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.9618e-04 - val_loss: 4.6210e-04\n",
      "\n",
      "Epoch 00197: loss did not improve from 0.00041\n",
      "Epoch 198/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.9877e-04 - val_loss: 4.4654e-04\n",
      "\n",
      "Epoch 00198: loss did not improve from 0.00041\n",
      "Epoch 199/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.0480e-04 - val_loss: 4.2733e-04\n",
      "\n",
      "Epoch 00199: loss improved from 0.00041 to 0.00039, saving model to .\\LSTM.h5\n",
      "Epoch 200/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.7040e-04 - val_loss: 4.1874e-04\n",
      "\n",
      "Epoch 00200: loss improved from 0.00039 to 0.00038, saving model to .\\LSTM.h5\n",
      "Epoch 201/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 3.6804e-04 - val_loss: 4.1093e-04\n",
      "\n",
      "Epoch 00201: loss improved from 0.00038 to 0.00037, saving model to .\\LSTM.h5\n",
      "Epoch 202/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.6335e-04 - val_loss: 4.0559e-04\n",
      "\n",
      "Epoch 00202: loss improved from 0.00037 to 0.00036, saving model to .\\LSTM.h5\n",
      "Epoch 203/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.7510e-04 - val_loss: 4.3686e-04\n",
      "\n",
      "Epoch 00203: loss did not improve from 0.00036\n",
      "Epoch 204/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 4.1361e-04 - val_loss: 4.7629e-04\n",
      "\n",
      "Epoch 00204: loss did not improve from 0.00036\n",
      "Epoch 205/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.0171e-04 - val_loss: 4.4106e-04\n",
      "\n",
      "Epoch 00205: loss did not improve from 0.00036\n",
      "Epoch 206/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.6446e-04 - val_loss: 3.8880e-04\n",
      "\n",
      "Epoch 00206: loss did not improve from 0.00036\n",
      "Epoch 207/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.2782e-04 - val_loss: 3.8803e-04\n",
      "\n",
      "Epoch 00207: loss improved from 0.00036 to 0.00034, saving model to .\\LSTM.h5\n",
      "Epoch 208/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.5563e-04 - val_loss: 3.6475e-04\n",
      "\n",
      "Epoch 00208: loss improved from 0.00034 to 0.00033, saving model to .\\LSTM.h5\n",
      "Epoch 209/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.2527e-04 - val_loss: 3.5939e-04\n",
      "\n",
      "Epoch 00209: loss improved from 0.00033 to 0.00032, saving model to .\\LSTM.h5\n",
      "Epoch 210/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.0209e-04 - val_loss: 3.7109e-04\n",
      "\n",
      "Epoch 00210: loss did not improve from 0.00032\n",
      "Epoch 211/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.4057e-04 - val_loss: 3.5936e-04\n",
      "\n",
      "Epoch 00211: loss did not improve from 0.00032\n",
      "Epoch 212/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.9950e-04 - val_loss: 3.5055e-04\n",
      "\n",
      "Epoch 00212: loss improved from 0.00032 to 0.00031, saving model to .\\LSTM.h5\n",
      "Epoch 213/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.3788e-04 - val_loss: 4.2480e-04\n",
      "\n",
      "Epoch 00213: loss did not improve from 0.00031\n",
      "Epoch 214/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4.1233e-04 - val_loss: 4.0024e-04\n",
      "\n",
      "Epoch 00214: loss did not improve from 0.00031\n",
      "Epoch 215/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 3.7526e-04 - val_loss: 3.3097e-04\n",
      "\n",
      "Epoch 00215: loss did not improve from 0.00031\n",
      "Epoch 216/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.9155e-04 - val_loss: 3.1074e-04\n",
      "\n",
      "Epoch 00216: loss improved from 0.00031 to 0.00028, saving model to .\\LSTM.h5\n",
      "Epoch 217/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.8573e-04 - val_loss: 3.1225e-04\n",
      "\n",
      "Epoch 00217: loss improved from 0.00028 to 0.00028, saving model to .\\LSTM.h5\n",
      "Epoch 218/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.5808e-04 - val_loss: 3.0377e-04\n",
      "\n",
      "Epoch 00218: loss improved from 0.00028 to 0.00027, saving model to .\\LSTM.h5\n",
      "Epoch 219/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.7436e-04 - val_loss: 3.0575e-04\n",
      "\n",
      "Epoch 00219: loss improved from 0.00027 to 0.00027, saving model to .\\LSTM.h5\n",
      "Epoch 220/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.4251e-04 - val_loss: 2.9501e-04\n",
      "\n",
      "Epoch 00220: loss improved from 0.00027 to 0.00027, saving model to .\\LSTM.h5\n",
      "Epoch 221/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 2.5211e-04 - val_loss: 2.8364e-04\n",
      "\n",
      "Epoch 00221: loss improved from 0.00027 to 0.00025, saving model to .\\LSTM.h5\n",
      "Epoch 222/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.4125e-04 - val_loss: 2.8092e-04\n",
      "\n",
      "Epoch 00222: loss improved from 0.00025 to 0.00025, saving model to .\\LSTM.h5\n",
      "Epoch 223/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.3681e-04 - val_loss: 2.8163e-04\n",
      "\n",
      "Epoch 00223: loss improved from 0.00025 to 0.00024, saving model to .\\LSTM.h5\n",
      "Epoch 224/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 2.4961e-04 - val_loss: 2.9662e-04\n",
      "\n",
      "Epoch 00224: loss did not improve from 0.00024\n",
      "Epoch 225/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.7944e-04 - val_loss: 2.7834e-04\n",
      "\n",
      "Epoch 00225: loss did not improve from 0.00024\n",
      "Epoch 226/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 2.4260e-04 - val_loss: 2.5983e-04\n",
      "\n",
      "Epoch 00226: loss improved from 0.00024 to 0.00024, saving model to .\\LSTM.h5\n",
      "Epoch 227/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2663e-04 - val_loss: 2.5091e-04\n",
      "\n",
      "Epoch 00227: loss improved from 0.00024 to 0.00023, saving model to .\\LSTM.h5\n",
      "Epoch 228/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2042e-04 - val_loss: 2.4676e-04\n",
      "\n",
      "Epoch 00228: loss improved from 0.00023 to 0.00022, saving model to .\\LSTM.h5\n",
      "Epoch 229/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.1728e-04 - val_loss: 2.6093e-04\n",
      "\n",
      "Epoch 00229: loss improved from 0.00022 to 0.00022, saving model to .\\LSTM.h5\n",
      "Epoch 230/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2673e-04 - val_loss: 2.5878e-04\n",
      "\n",
      "Epoch 00230: loss did not improve from 0.00022\n",
      "Epoch 231/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.2374e-04 - val_loss: 2.4387e-04\n",
      "\n",
      "Epoch 00231: loss did not improve from 0.00022\n",
      "Epoch 232/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9585e-04 - val_loss: 2.3519e-04\n",
      "\n",
      "Epoch 00232: loss improved from 0.00022 to 0.00021, saving model to .\\LSTM.h5\n",
      "Epoch 233/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0112e-04 - val_loss: 2.2938e-04\n",
      "\n",
      "Epoch 00233: loss improved from 0.00021 to 0.00020, saving model to .\\LSTM.h5\n",
      "Epoch 234/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.9803e-04 - val_loss: 2.3537e-04\n",
      "\n",
      "Epoch 00234: loss did not improve from 0.00020\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 17ms/step - loss: 1.8852e-04 - val_loss: 2.1585e-04\n",
      "\n",
      "Epoch 00235: loss improved from 0.00020 to 0.00020, saving model to .\\LSTM.h5\n",
      "Epoch 236/1000\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.9461e-0 - 0s 18ms/step - loss: 1.8885e-04 - val_loss: 2.2499e-04\n",
      "\n",
      "Epoch 00236: loss improved from 0.00020 to 0.00019, saving model to .\\LSTM.h5\n",
      "Epoch 237/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.9806e-04 - val_loss: 2.2480e-04\n",
      "\n",
      "Epoch 00237: loss did not improve from 0.00019\n",
      "Epoch 238/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 2.0175e-04 - val_loss: 2.0542e-04\n",
      "\n",
      "Epoch 00238: loss did not improve from 0.00019\n",
      "Epoch 239/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.9176e-04 - val_loss: 2.0466e-04\n",
      "\n",
      "Epoch 00239: loss improved from 0.00019 to 0.00018, saving model to .\\LSTM.h5\n",
      "Epoch 240/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.9634e-04 - val_loss: 2.2951e-04\n",
      "\n",
      "Epoch 00240: loss did not improve from 0.00018\n",
      "Epoch 241/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.9767e-04 - val_loss: 2.1487e-04\n",
      "\n",
      "Epoch 00241: loss did not improve from 0.00018\n",
      "Epoch 242/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.8596e-04 - val_loss: 1.9384e-04\n",
      "\n",
      "Epoch 00242: loss improved from 0.00018 to 0.00018, saving model to .\\LSTM.h5\n",
      "Epoch 243/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.6264e-04 - val_loss: 2.2468e-04\n",
      "\n",
      "Epoch 00243: loss did not improve from 0.00018\n",
      "Epoch 244/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.0911e-04 - val_loss: 2.1317e-04\n",
      "\n",
      "Epoch 00244: loss did not improve from 0.00018\n",
      "Epoch 245/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 1.8122e-04 - val_loss: 1.9039e-04\n",
      "\n",
      "Epoch 00245: loss did not improve from 0.00018\n",
      "Epoch 246/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6979e-04 - val_loss: 1.8462e-04\n",
      "\n",
      "Epoch 00246: loss improved from 0.00018 to 0.00016, saving model to .\\LSTM.h5\n",
      "Epoch 247/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5394e-04 - val_loss: 1.8028e-04\n",
      "\n",
      "Epoch 00247: loss improved from 0.00016 to 0.00016, saving model to .\\LSTM.h5\n",
      "Epoch 248/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.6595e-04 - val_loss: 1.7788e-04\n",
      "\n",
      "Epoch 00248: loss improved from 0.00016 to 0.00016, saving model to .\\LSTM.h5\n",
      "Epoch 249/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.6089e-04 - val_loss: 1.7279e-04\n",
      "\n",
      "Epoch 00249: loss did not improve from 0.00016\n",
      "Epoch 250/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.4347e-04 - val_loss: 1.6761e-04\n",
      "\n",
      "Epoch 00250: loss improved from 0.00016 to 0.00015, saving model to .\\LSTM.h5\n",
      "Epoch 251/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5552e-04 - val_loss: 1.6249e-04\n",
      "\n",
      "Epoch 00251: loss improved from 0.00015 to 0.00015, saving model to .\\LSTM.h5\n",
      "Epoch 252/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.3944e-04 - val_loss: 1.6457e-04\n",
      "\n",
      "Epoch 00252: loss improved from 0.00015 to 0.00014, saving model to .\\LSTM.h5\n",
      "Epoch 253/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3314e-04 - val_loss: 1.8087e-04\n",
      "\n",
      "Epoch 00253: loss did not improve from 0.00014\n",
      "Epoch 254/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.7148e-04 - val_loss: 1.9592e-04\n",
      "\n",
      "Epoch 00254: loss did not improve from 0.00014\n",
      "Epoch 255/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.7653e-04 - val_loss: 1.7194e-04\n",
      "\n",
      "Epoch 00255: loss did not improve from 0.00014\n",
      "Epoch 256/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.5992e-04 - val_loss: 1.5035e-04\n",
      "\n",
      "Epoch 00256: loss did not improve from 0.00014\n",
      "Epoch 257/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.3191e-04 - val_loss: 1.5531e-04\n",
      "\n",
      "Epoch 00257: loss improved from 0.00014 to 0.00013, saving model to .\\LSTM.h5\n",
      "Epoch 258/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.3406e-04 - val_loss: 1.6119e-04\n",
      "\n",
      "Epoch 00258: loss did not improve from 0.00013\n",
      "Epoch 259/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.4379e-04 - val_loss: 1.4505e-04\n",
      "\n",
      "Epoch 00259: loss did not improve from 0.00013\n",
      "Epoch 260/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.3206e-04 - val_loss: 1.3661e-04\n",
      "\n",
      "Epoch 00260: loss improved from 0.00013 to 0.00013, saving model to .\\LSTM.h5\n",
      "Epoch 261/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.2836e-04 - val_loss: 1.5943e-04\n",
      "\n",
      "Epoch 00261: loss improved from 0.00013 to 0.00013, saving model to .\\LSTM.h5\n",
      "Epoch 262/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.4991e-04 - val_loss: 1.5837e-04\n",
      "\n",
      "Epoch 00262: loss did not improve from 0.00013\n",
      "Epoch 263/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3550e-04 - val_loss: 1.4551e-04\n",
      "\n",
      "Epoch 00263: loss did not improve from 0.00013\n",
      "Epoch 264/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.2177e-04 - val_loss: 1.2900e-04\n",
      "\n",
      "Epoch 00264: loss improved from 0.00013 to 0.00012, saving model to .\\LSTM.h5\n",
      "Epoch 265/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.1780e-04 - val_loss: 1.2373e-04\n",
      "\n",
      "Epoch 00265: loss improved from 0.00012 to 0.00011, saving model to .\\LSTM.h5\n",
      "Epoch 266/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1289e-04 - val_loss: 1.2426e-04\n",
      "\n",
      "Epoch 00266: loss did not improve from 0.00011\n",
      "Epoch 267/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1138e-04 - val_loss: 1.2134e-04\n",
      "\n",
      "Epoch 00267: loss improved from 0.00011 to 0.00011, saving model to .\\LSTM.h5\n",
      "Epoch 268/1000\n",
      "5/5 [==============================] - ETA: 0s - loss: 1.0381e-0 - 0s 16ms/step - loss: 1.0857e-04 - val_loss: 1.2649e-04\n",
      "\n",
      "Epoch 00268: loss did not improve from 0.00011\n",
      "Epoch 269/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.2195e-04 - val_loss: 1.2305e-04\n",
      "\n",
      "Epoch 00269: loss did not improve from 0.00011\n",
      "Epoch 270/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1442e-04 - val_loss: 1.2453e-04\n",
      "\n",
      "Epoch 00270: loss did not improve from 0.00011\n",
      "Epoch 271/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0994e-04 - val_loss: 1.1992e-04\n",
      "\n",
      "Epoch 00271: loss improved from 0.00011 to 0.00011, saving model to .\\LSTM.h5\n",
      "Epoch 272/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.0270e-04 - val_loss: 1.2588e-04\n",
      "\n",
      "Epoch 00272: loss improved from 0.00011 to 0.00011, saving model to .\\LSTM.h5\n",
      "Epoch 273/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0967e-04 - val_loss: 1.2831e-04\n",
      "\n",
      "Epoch 00273: loss did not improve from 0.00011\n",
      "Epoch 274/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 1.1794e-04 - val_loss: 1.1464e-04\n",
      "\n",
      "Epoch 00274: loss did not improve from 0.00011\n",
      "Epoch 275/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0617e-04 - val_loss: 1.0657e-04\n",
      "\n",
      "Epoch 00275: loss improved from 0.00011 to 0.00010, saving model to .\\LSTM.h5\n",
      "Epoch 276/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.8372e-05 - val_loss: 1.0525e-04\n",
      "\n",
      "Epoch 00276: loss improved from 0.00010 to 0.00010, saving model to .\\LSTM.h5\n",
      "Epoch 277/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.4815e-05 - val_loss: 1.1141e-04\n",
      "\n",
      "Epoch 00277: loss improved from 0.00010 to 0.00009, saving model to .\\LSTM.h5\n",
      "Epoch 278/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 9.8951e-05 - val_loss: 1.1749e-04\n",
      "\n",
      "Epoch 00278: loss did not improve from 0.00009\n",
      "Epoch 279/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 9.6552e-05 - val_loss: 1.0884e-04\n",
      "\n",
      "Epoch 00279: loss did not improve from 0.00009\n",
      "Epoch 280/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.9256e-05 - val_loss: 9.8296e-05\n",
      "\n",
      "Epoch 00280: loss did not improve from 0.00009\n",
      "Epoch 281/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.6931e-05 - val_loss: 1.1163e-04\n",
      "\n",
      "Epoch 00281: loss improved from 0.00009 to 0.00009, saving model to .\\LSTM.h5\n",
      "Epoch 282/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1199e-04 - val_loss: 1.1859e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00282: loss did not improve from 0.00009\n",
      "Epoch 283/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1223e-04 - val_loss: 9.9089e-05\n",
      "\n",
      "Epoch 00283: loss did not improve from 0.00009\n",
      "Epoch 284/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.7556e-05 - val_loss: 1.0159e-04\n",
      "\n",
      "Epoch 00284: loss improved from 0.00009 to 0.00009, saving model to .\\LSTM.h5\n",
      "Epoch 285/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.4656e-05 - val_loss: 1.0844e-04\n",
      "\n",
      "Epoch 00285: loss did not improve from 0.00009\n",
      "Epoch 286/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.0840e-04 - val_loss: 1.0294e-04\n",
      "\n",
      "Epoch 00286: loss did not improve from 0.00009\n",
      "Epoch 287/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.2340e-05 - val_loss: 9.2108e-05\n",
      "\n",
      "Epoch 00287: loss improved from 0.00009 to 0.00009, saving model to .\\LSTM.h5\n",
      "Epoch 288/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 8.3090e-05 - val_loss: 9.4297e-05\n",
      "\n",
      "Epoch 00288: loss improved from 0.00009 to 0.00008, saving model to .\\LSTM.h5\n",
      "Epoch 289/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 9.1219e-05 - val_loss: 9.2198e-05\n",
      "\n",
      "Epoch 00289: loss did not improve from 0.00008\n",
      "Epoch 290/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.5167e-05 - val_loss: 8.8259e-05\n",
      "\n",
      "Epoch 00290: loss improved from 0.00008 to 0.00008, saving model to .\\LSTM.h5\n",
      "Epoch 291/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.7131e-05 - val_loss: 8.4838e-05\n",
      "\n",
      "Epoch 00291: loss improved from 0.00008 to 0.00008, saving model to .\\LSTM.h5\n",
      "Epoch 292/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.9678e-05 - val_loss: 8.6524e-05\n",
      "\n",
      "Epoch 00292: loss improved from 0.00008 to 0.00008, saving model to .\\LSTM.h5\n",
      "Epoch 293/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.5792e-05 - val_loss: 8.7905e-05\n",
      "\n",
      "Epoch 00293: loss did not improve from 0.00008\n",
      "Epoch 294/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 7.8310e-05 - val_loss: 8.6795e-05\n",
      "\n",
      "Epoch 00294: loss improved from 0.00008 to 0.00008, saving model to .\\LSTM.h5\n",
      "Epoch 295/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.3901e-05 - val_loss: 9.5370e-05\n",
      "\n",
      "Epoch 00295: loss did not improve from 0.00008\n",
      "Epoch 296/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 8.5244e-05 - val_loss: 9.1412e-05\n",
      "\n",
      "Epoch 00296: loss did not improve from 0.00008\n",
      "Epoch 297/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 8.3308e-05 - val_loss: 7.7648e-05\n",
      "\n",
      "Epoch 00297: loss improved from 0.00008 to 0.00008, saving model to .\\LSTM.h5\n",
      "Epoch 298/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.8450e-05 - val_loss: 8.3908e-05\n",
      "\n",
      "Epoch 00298: loss improved from 0.00008 to 0.00008, saving model to .\\LSTM.h5\n",
      "Epoch 299/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 8.0273e-05 - val_loss: 7.9488e-05\n",
      "\n",
      "Epoch 00299: loss did not improve from 0.00008\n",
      "Epoch 300/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 7.7243e-05 - val_loss: 7.6714e-05\n",
      "\n",
      "Epoch 00300: loss did not improve from 0.00008\n",
      "Epoch 301/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 6.9648e-05 - val_loss: 7.2883e-05\n",
      "\n",
      "Epoch 00301: loss improved from 0.00008 to 0.00007, saving model to .\\LSTM.h5\n",
      "Epoch 302/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.7088e-05 - val_loss: 8.2385e-05\n",
      "\n",
      "Epoch 00302: loss did not improve from 0.00007\n",
      "Epoch 303/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 7.5788e-05 - val_loss: 7.6523e-05\n",
      "\n",
      "Epoch 00303: loss did not improve from 0.00007\n",
      "Epoch 304/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 7.2844e-05 - val_loss: 7.0523e-05\n",
      "\n",
      "Epoch 00304: loss improved from 0.00007 to 0.00007, saving model to .\\LSTM.h5\n",
      "Epoch 305/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.5209e-05 - val_loss: 6.8660e-05\n",
      "\n",
      "Epoch 00305: loss improved from 0.00007 to 0.00006, saving model to .\\LSTM.h5\n",
      "Epoch 306/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.3939e-05 - val_loss: 6.6775e-05\n",
      "\n",
      "Epoch 00306: loss did not improve from 0.00006\n",
      "Epoch 307/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.9320e-05 - val_loss: 6.4895e-05\n",
      "\n",
      "Epoch 00307: loss improved from 0.00006 to 0.00006, saving model to .\\LSTM.h5\n",
      "Epoch 308/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.2102e-05 - val_loss: 6.4596e-05\n",
      "\n",
      "Epoch 00308: loss improved from 0.00006 to 0.00006, saving model to .\\LSTM.h5\n",
      "Epoch 309/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 5.8788e-05 - val_loss: 6.3994e-05\n",
      "\n",
      "Epoch 00309: loss improved from 0.00006 to 0.00006, saving model to .\\LSTM.h5\n",
      "Epoch 310/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0320e-05 - val_loss: 6.3036e-05\n",
      "\n",
      "Epoch 00310: loss improved from 0.00006 to 0.00006, saving model to .\\LSTM.h5\n",
      "Epoch 311/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.2058e-05 - val_loss: 6.6035e-05\n",
      "\n",
      "Epoch 00311: loss improved from 0.00006 to 0.00006, saving model to .\\LSTM.h5\n",
      "Epoch 312/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.0429e-05 - val_loss: 6.8999e-05\n",
      "\n",
      "Epoch 00312: loss did not improve from 0.00006\n",
      "Epoch 313/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.3108e-05 - val_loss: 7.2642e-05\n",
      "\n",
      "Epoch 00313: loss did not improve from 0.00006\n",
      "Epoch 314/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.3301e-05 - val_loss: 6.4287e-05\n",
      "\n",
      "Epoch 00314: loss did not improve from 0.00006\n",
      "Epoch 315/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.8524e-05 - val_loss: 5.9197e-05\n",
      "\n",
      "Epoch 00315: loss improved from 0.00006 to 0.00006, saving model to .\\LSTM.h5\n",
      "Epoch 316/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 5.5221e-05 - val_loss: 6.3771e-05\n",
      "\n",
      "Epoch 00316: loss did not improve from 0.00006\n",
      "Epoch 317/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.0165e-05 - val_loss: 6.4209e-05\n",
      "\n",
      "Epoch 00317: loss did not improve from 0.00006\n",
      "Epoch 318/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 5.7894e-05 - val_loss: 7.4122e-05\n",
      "\n",
      "Epoch 00318: loss did not improve from 0.00006\n",
      "Epoch 319/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 6.5294e-05 - val_loss: 7.6949e-05\n",
      "\n",
      "Epoch 00319: loss did not improve from 0.00006\n",
      "Epoch 320/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 6.7810e-05 - val_loss: 6.2601e-05\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00320: loss did not improve from 0.00006\n",
      "Epoch 00320: early stopping\n",
      "time: 33.47191023826599\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 정의\n",
    "model = Sequential()\n",
    "# units=10: rnn 계층에 존재하는 전체 뉴런수\n",
    "# return_sequences=False: 은닉 상태값을 출력할지 결정\n",
    "# False: 마지막 시점의 메모리 셀에서만 결과를 출력, True: 모든 rnn 계산과정에서 결과를 출력\n",
    "# return_sequences는 다층 구조의 rnn 모델이나 one-to-many, many-to-many구조에서 사용\n",
    "# input_shape=(n_timesteps, n_features): 입력 시퀀스 길이, 변수의 갯수\n",
    "model.add(LSTM(units=10, return_sequences=False, input_shape=(n_timesteps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 학습\n",
    "es = EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('./LSTM.h5', monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "start = time.time()\n",
    "hist = model.fit(train_x, train_y, validation_split=0.3, \n",
    "                 shuffle=True, epochs=1000, callbacks=[es, mc])\n",
    "print('time:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "# plt.figure(figsize=(6,4)) # ERROR\n",
    "fig.set_size_inches(10, 5)  # 챠트 크기 설정\n",
    "\n",
    "# 왼쪽 y 축 설정\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.0]) # 값을 반영하여 변경\n",
    "\n",
    "# 축 레이블 설정\n",
    "loss_ax.set_xlabel('epoch')  # 학습 횟수\n",
    "loss_ax.set_ylabel('loss')   # 오차\n",
    "\n",
    "loss_ax.legend(loc='upper left') # 오차 레이블 위치\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 생성\n",
    "test_x = np.arange(0, 10, 0.1)\n",
    "# test_x = np.arange(10, 20, 0.1)\n",
    "calc_y = np.sin(test_x) # 테스트 정답 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yp: [[0.98495024]]\n",
      "(15,) (1, 1) 0 15\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024]\n",
      "yp: [[0.9822517]]\n",
      "(16,) (1, 1) 1 16\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517 ]\n",
      "yp: [[0.970487]]\n",
      "(17,) (1, 1) 2 17\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487  ]\n",
      "yp: [[0.9497356]]\n",
      "(18,) (1, 1) 3 18\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558]\n",
      "yp: [[0.92045844]]\n",
      "(19,) (1, 1) 4 19\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844]\n",
      "yp: [[0.88313335]]\n",
      "(20,) (1, 1) 5 20\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335]\n",
      "yp: [[0.83813745]]\n",
      "(21,) (1, 1) 6 21\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745]\n",
      "yp: [[0.78571516]]\n",
      "(22,) (1, 1) 7 22\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516]\n",
      "yp: [[0.7260013]]\n",
      "(23,) (1, 1) 8 23\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132]\n",
      "yp: [[0.6590896]]\n",
      "(24,) (1, 1) 9 24\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962]\n",
      "yp: [[0.58512574]]\n",
      "(25,) (1, 1) 10 25\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962 0.58512574]\n",
      "yp: [[0.5044129]]\n",
      "(26,) (1, 1) 11 26\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962 0.58512574 0.50441289]\n",
      "yp: [[0.41750813]]\n",
      "(27,) (1, 1) 12 27\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962 0.58512574 0.50441289 0.41750813]\n",
      "yp: [[0.32528558]]\n",
      "(28,) (1, 1) 13 28\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962 0.58512574 0.50441289 0.41750813 0.32528558]\n",
      "yp: [[0.22893976]]\n",
      "(29,) (1, 1) 14 29\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962 0.58512574 0.50441289 0.41750813 0.32528558 0.22893976]\n",
      "yp: [[0.12991048]]\n",
      "(30,) (1, 1) 15 30\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962 0.58512574 0.50441289 0.41750813 0.32528558 0.22893976\n",
      " 0.12991048]\n",
      "yp: [[0.02988074]]\n",
      "(31,) (1, 1) 16 31\n",
      "[0.         0.09983342 0.19866933 0.29552021 0.38941834 0.47942554\n",
      " 0.56464247 0.64421769 0.71735609 0.78332691 0.84147098 0.89120736\n",
      " 0.93203909 0.96355819 0.98544973 0.98495024 0.9822517  0.970487\n",
      " 0.94973558 0.92045844 0.88313335 0.83813745 0.78571516 0.72600132\n",
      " 0.65908962 0.58512574 0.50441289 0.41750813 0.32528558 0.22893976\n",
      " 0.12991048 0.02988074]\n",
      "yp: [[-0.06945793]]\n",
      "(32,) (1, 1) 17 32\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793]\n",
      "yp: [[-0.16672501]]\n",
      "(33,) (1, 1) 18 33\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501]\n",
      "yp: [[-0.26095474]]\n",
      "(34,) (1, 1) 19 34\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474]\n",
      "yp: [[-0.3516276]]\n",
      "(35,) (1, 1) 20 35\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759]\n",
      "yp: [[-0.4385727]]\n",
      "(36,) (1, 1) 21 36\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727 ]\n",
      "yp: [[-0.52176327]]\n",
      "(37,) (1, 1) 22 37\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327]\n",
      "yp: [[-0.6010555]]\n",
      "(38,) (1, 1) 23 38\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555 ]\n",
      "yp: [[-0.6759443]]\n",
      "(39,) (1, 1) 24 39\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yp: [[-0.74543357]]\n",
      "(40,) (1, 1) 25 40\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357]\n",
      "yp: [[-0.80809814]]\n",
      "(41,) (1, 1) 26 41\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814]\n",
      "yp: [[-0.8623517]]\n",
      "(42,) (1, 1) 27 42\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172]\n",
      "yp: [[-0.9067981]]\n",
      "(43,) (1, 1) 28 43\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812]\n",
      "yp: [[-0.9404965]]\n",
      "(44,) (1, 1) 29 44\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965 ]\n",
      "yp: [[-0.96304137]]\n",
      "(45,) (1, 1) 30 45\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137]\n",
      "yp: [[-0.97449046]]\n",
      "(46,) (1, 1) 31 46\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046]\n",
      "yp: [[-0.97523135]]\n",
      "(47,) (1, 1) 32 47\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135]\n",
      "yp: [[-0.9658473]]\n",
      "(48,) (1, 1) 33 48\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731]\n",
      "yp: [[-0.94700366]]\n",
      "(49,) (1, 1) 34 49\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366]\n",
      "yp: [[-0.9193587]]\n",
      "(50,) (1, 1) 35 50\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867]\n",
      "yp: [[-0.88350207]]\n",
      "(51,) (1, 1) 36 51\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207]\n",
      "yp: [[-0.8399239]]\n",
      "(52,) (1, 1) 37 52\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392]\n",
      "yp: [[-0.7890163]]\n",
      "(53,) (1, 1) 38 53\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yp: [[-0.7311029]]\n",
      "(54,) (1, 1) 39 54\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288]\n",
      "yp: [[-0.66648376]]\n",
      "(55,) (1, 1) 40 55\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376]\n",
      "yp: [[-0.5954874]]\n",
      "(56,) (1, 1) 41 56\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742]\n",
      "yp: [[-0.5185166]]\n",
      "(57,) (1, 1) 42 57\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166 ]\n",
      "yp: [[-0.43608335]]\n",
      "(58,) (1, 1) 43 58\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335]\n",
      "yp: [[-0.34882876]]\n",
      "(59,) (1, 1) 44 59\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876]\n",
      "yp: [[-0.25752002]]\n",
      "(60,) (1, 1) 45 60\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002]\n",
      "yp: [[-0.1630246]]\n",
      "(61,) (1, 1) 46 61\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246 ]\n",
      "yp: [[-0.06626564]]\n",
      "(62,) (1, 1) 47 62\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564]\n",
      "yp: [[0.03182578]]\n",
      "(63,) (1, 1) 48 63\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578]\n",
      "yp: [[0.1303383]]\n",
      "(64,) (1, 1) 49 64\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383 ]\n",
      "yp: [[0.22837074]]\n",
      "(65,) (1, 1) 50 65\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074]\n",
      "yp: [[0.3249883]]\n",
      "(66,) (1, 1) 51 66\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831]\n",
      "yp: [[0.41916817]]\n",
      "(67,) (1, 1) 52 67\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817]\n",
      "yp: [[0.50977165]]\n",
      "(68,) (1, 1) 53 68\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yp: [[0.59556264]]\n",
      "(69,) (1, 1) 54 69\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264]\n",
      "yp: [[0.6752698]]\n",
      "(70,) (1, 1) 55 70\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978]\n",
      "yp: [[0.74766934]]\n",
      "(71,) (1, 1) 56 71\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934]\n",
      "yp: [[0.8116652]]\n",
      "(72,) (1, 1) 57 72\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518]\n",
      "yp: [[0.86635154]]\n",
      "(73,) (1, 1) 58 73\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154]\n",
      "yp: [[0.9110545]]\n",
      "(74,) (1, 1) 59 74\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449]\n",
      "yp: [[0.9453543]]\n",
      "(75,) (1, 1) 60 75\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428]\n",
      "yp: [[0.9690859]]\n",
      "(76,) (1, 1) 61 76\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587]\n",
      "yp: [[0.98232305]]\n",
      "(77,) (1, 1) 62 77\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305]\n",
      "yp: [[0.98534447]]\n",
      "(78,) (1, 1) 63 78\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447]\n",
      "yp: [[0.9785842]]\n",
      "(79,) (1, 1) 64 79\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417]\n",
      "yp: [[0.9625718]]\n",
      "(80,) (1, 1) 65 80\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718 ]\n",
      "yp: [[0.9378626]]\n",
      "(81,) (1, 1) 66 81\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yp: [[0.90496916]]\n",
      "(82,) (1, 1) 67 82\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916]\n",
      "yp: [[0.8643056]]\n",
      "(83,) (1, 1) 68 83\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562]\n",
      "yp: [[0.81615895]]\n",
      "(84,) (1, 1) 69 84\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895]\n",
      "yp: [[0.76069766]]\n",
      "(85,) (1, 1) 70 85\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766]\n",
      "yp: [[0.69801486]]\n",
      "(86,) (1, 1) 71 86\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486]\n",
      "yp: [[0.62820476]]\n",
      "(87,) (1, 1) 72 87\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476]\n",
      "yp: [[0.55145556]]\n",
      "(88,) (1, 1) 73 88\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556]\n",
      "yp: [[0.4681505]]\n",
      "(89,) (1, 1) 74 89\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505 ]\n",
      "yp: [[0.3789575]]\n",
      "(90,) (1, 1) 75 90\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yp: [[0.28488612]]\n",
      "(91,) (1, 1) 76 91\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612]\n",
      "yp: [[0.18728358]]\n",
      "(92,) (1, 1) 77 92\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358]\n",
      "yp: [[0.08775049]]\n",
      "(93,) (1, 1) 78 93\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358  0.08775049]\n",
      "yp: [[-0.0120289]]\n",
      "(94,) (1, 1) 79 94\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358  0.08775049 -0.0120289 ]\n",
      "yp: [[-0.11050205]]\n",
      "(95,) (1, 1) 80 95\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358  0.08775049 -0.0120289  -0.11050205]\n",
      "yp: [[-0.20644762]]\n",
      "(96,) (1, 1) 81 96\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358  0.08775049 -0.0120289  -0.11050205\n",
      " -0.20644762]\n",
      "yp: [[-0.29908642]]\n",
      "(97,) (1, 1) 82 97\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358  0.08775049 -0.0120289  -0.11050205\n",
      " -0.20644762 -0.29908642]\n",
      "yp: [[-0.38806137]]\n",
      "(98,) (1, 1) 83 98\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358  0.08775049 -0.0120289  -0.11050205\n",
      " -0.20644762 -0.29908642 -0.38806137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yp: [[-0.47329074]]\n",
      "(99,) (1, 1) 84 99\n",
      "[ 0.          0.09983342  0.19866933  0.29552021  0.38941834  0.47942554\n",
      "  0.56464247  0.64421769  0.71735609  0.78332691  0.84147098  0.89120736\n",
      "  0.93203909  0.96355819  0.98544973  0.98495024  0.9822517   0.970487\n",
      "  0.94973558  0.92045844  0.88313335  0.83813745  0.78571516  0.72600132\n",
      "  0.65908962  0.58512574  0.50441289  0.41750813  0.32528558  0.22893976\n",
      "  0.12991048  0.02988074 -0.06945793 -0.16672501 -0.26095474 -0.35162759\n",
      " -0.4385727  -0.52176327 -0.6010555  -0.67594433 -0.74543357 -0.80809814\n",
      " -0.86235172 -0.90679812 -0.9404965  -0.96304137 -0.97449046 -0.97523135\n",
      " -0.96584731 -0.94700366 -0.91935867 -0.88350207 -0.83992392 -0.78901631\n",
      " -0.73110288 -0.66648376 -0.59548742 -0.5185166  -0.43608335 -0.34882876\n",
      " -0.25752002 -0.1630246  -0.06626564  0.03182578  0.1303383   0.22837074\n",
      "  0.32498831  0.41916817  0.50977165  0.59556264  0.67526978  0.74766934\n",
      "  0.81166518  0.86635154  0.91105449  0.94535428  0.96908587  0.98232305\n",
      "  0.98534447  0.97858417  0.9625718   0.93786258  0.90496916  0.86430562\n",
      "  0.81615895  0.76069766  0.69801486  0.62820476  0.55145556  0.4681505\n",
      "  0.37895751  0.28488612  0.18728358  0.08775049 -0.0120289  -0.11050205\n",
      " -0.20644762 -0.29908642 -0.38806137 -0.47329074]\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델 예측 및 로그 저장\n",
    "test_y = calc_y[:n_timesteps] # 0:15\n",
    "for i in range(len(test_x) - n_timesteps):\n",
    "    net_input = test_y[i : i + n_timesteps]\n",
    "    net_input = net_input.reshape((1, n_timesteps, n_features)) # 1, 15, 1\n",
    "    yp = model.predict(net_input, verbose=0) # 모델 사용\n",
    "    print('yp:', yp)\n",
    "    print(test_y.shape, yp.shape, i, i + n_timesteps)\n",
    "    # test_y.shape: (15,), yp.shape: (1, 1), i: 0, i + n_timesteps: 15\n",
    "    test_y = np.append(test_y, yp)\n",
    "    print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정답 그래프 출력\n",
    "plt.plot(test_x, calc_y, label=\"ground truth\", color=\"orange\") # target\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 정답과 예측 결과 출력\n",
    "plt.plot(test_x, calc_y, label=\"ground truth\", color=\"orange\") # target\n",
    "plt.plot(test_x, test_y, label=\"predicitons\", color=\"blue\") # predict\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
