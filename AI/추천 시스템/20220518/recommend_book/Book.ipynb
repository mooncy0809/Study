{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추천 시스템 제작(Book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from tensorflow.keras.models import Sequential  # class\n",
    "from tensorflow.keras.models import load_model  # model 사용\n",
    "from tensorflow.keras.layers import Dense       # 전결합\n",
    "from tensorflow.keras.layers import Dropout     # 특정 node를 사용안함.\n",
    "from tensorflow.keras.callbacks import EarlyStopping   # 학습 자동 중지\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint # 우수한 학습 모델 파일 저장\n",
    "from tensorflow.keras import regularizers \n",
    "from tensorflow.keras.utils import to_categorical   # one-hot 엔코딩\n",
    "from tensorflow.keras.optimizers import Adam    # 가중치, bias 최적화\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split # 학습셋과 테스트셋의 분리 지원\n",
    "from sklearn.model_selection import StratifiedKFold  # K겹 교차 검증\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "import platform \n",
    "\n",
    "if (platform.system() == 'Windows'):  # Windows, Linux, Darwin\n",
    "    rc('font', family=font_manager.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name())\n",
    "    path = '.' # Local\n",
    "else:    \n",
    "    plt.rc('font', family='NanumBarunGothic')  # Ubuntu 18.04 기준 한글 처리\n",
    "    path = '/content/drive/My Drive/ai7/dnn/recommendation' # Colab\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12         # 글자 크기\n",
    "# plt.rcParams[\"figure.figsize\"] = (10, 4) # 10:4의 그래프 비율\n",
    "plt.rcParams['axes.unicode_minus'] = False  # minus 부호는 unicode 적용시 한글이 깨짐으로 설정\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(99, 10)\n"
     ]
    }
   ],
   "source": [
    "# header가 있을경우 skiprows=1 선언\n",
    "# data = np.loadtxt(path + '/train1.csv', delimiter=',', skiprows=1, dtype=np.float64)  # 특성이 작은 데이터\n",
    "# data = np.loadtxt(path + '/train2.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가\n",
    "# data = np.loadtxt(path + '/train3.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가\n",
    "data = np.loadtxt(path + '/train4.csv', delimiter=',', skiprows=1, dtype=np.float64)   # 특성이 작은 데이터의 예외 추가\n",
    "print(type(data))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 9)\n",
      "(99,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터와 class의 분리\n",
    "# 1,1,1,0,0,0,0,0,0,0\n",
    "# 0: 개발 관련 도서\n",
    "# 1: 해외 여행 관련 도서\n",
    "# 2: 소설 관련 도서\n",
    "X = data[:, 0:9]  # 0 ~ 8\n",
    "print(X.shape)\n",
    "Y = data[:, 9]    # 10 번째 데이터, class의 분리\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 0. 1. 2. 0. 1. 2. 0. 1. 2. 0. 1. 2. 0. 1. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0.\n",
      " 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "Y = Y.astype('int') # 정수로 형변환\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 0: 개발 관련 도서\n",
    "# 1: 해외 여행 관련 도서\n",
    "# 2: 소설 관련 도서\n",
    "Y_encoded = to_categorical(Y) # one-hot-encoding\n",
    "\n",
    "print(Y_encoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 0.1 0.1 0.8 0.1 0.1 0.8 0.1 0.1]\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "(18, 3)\n"
     ]
    }
   ],
   "source": [
    "# train_test_split 분할을 통한 훈련, 검증, 테스트 데이터의 분리\n",
    "seed = 0\n",
    "# 90%: 분할대기, 10%: 테스트\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(X, Y_encoded,\n",
    "                                           stratify=Y_encoded,\n",
    "                                           test_size=0.1,\n",
    "                                           random_state=seed)\n",
    "# 나머지 데이터 90%를 분할, 80%: 훈련, 20%: 검증\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all,\n",
    "                                           stratify=y_train_all,\n",
    "                                           test_size=0.2,\n",
    "                                           random_state=seed)\n",
    "\n",
    "print(y_val)\n",
    "print(y_val.shape)\n",
    "# Iris-setosa: 0, Iris-versicolor: 1, Iris-virginica: 2\n",
    "# (7, 3): 7개의 데이터가 입력되어 한건당 3가지에 속할 확률이 출력됨으로 7행 3열이됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_test.shape)\n",
    "# (4, 3): 4개의 데이터가 입력되어 한건당 3가지에 속할 확률이 출력됨으로 4행 3열이됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 5)                 50        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 68\n",
      "Trainable params: 68\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "71/71 [==============================] - 1s 4ms/step - loss: 1.0587 - accuracy: 0.4493 - val_loss: 0.7733 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88889, saving model to .\\Book.h5\n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6787 - accuracy: 0.8737 - val_loss: 0.3445 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.88889 to 1.00000, saving model to .\\Book.h5\n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2919 - accuracy: 0.9932 - val_loss: 0.1779 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 1.00000\n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 1.00000\n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 1.00000\n",
      "Epoch 7/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 1.00000\n",
      "Epoch 8/50\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 1.00000\n",
      "Epoch 9/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 1.00000\n",
      "Epoch 10/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 1.00000\n",
      "Epoch 11/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 1.00000\n",
      "Epoch 12/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 1.00000\n",
      "Epoch 13/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 1.00000\n",
      "Epoch 14/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 1.00000\n",
      "Epoch 15/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 1.00000\n",
      "Epoch 16/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 1.00000\n",
      "Epoch 17/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 1.00000\n",
      "Epoch 18/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 1.00000\n",
      "Epoch 19/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 1.00000\n",
      "Epoch 20/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 1.00000\n",
      "Epoch 21/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 1.00000\n",
      "Epoch 22/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 1.00000\n",
      "Epoch 23/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 1.00000\n",
      "Epoch 24/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 1.00000\n",
      "Epoch 25/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 1.00000\n",
      "Epoch 26/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 1.00000\n",
      "Epoch 27/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 1.00000\n",
      "Epoch 28/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 1.00000\n",
      "Epoch 29/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 1.00000\n",
      "Epoch 30/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 1.00000\n",
      "Epoch 31/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 1.00000\n",
      "Epoch 32/50\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 9.7730e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 1.00000\n",
      "Epoch 33/50\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 9.7866e-04 - accuracy: 1.0000 - val_loss: 9.2000e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 1.00000\n",
      "Epoch 34/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 8.5061e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 1.00000\n",
      "Epoch 35/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 7.9448e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 1.00000\n",
      "Epoch 36/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 8.0923e-04 - accuracy: 1.0000 - val_loss: 7.4179e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 1.00000\n",
      "Epoch 37/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 8.2388e-04 - accuracy: 1.0000 - val_loss: 6.9447e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 1.00000\n",
      "Epoch 38/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 7.0397e-04 - accuracy: 1.0000 - val_loss: 6.4854e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 1.00000\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step - loss: 6.5814e-04 - accuracy: 1.0000 - val_loss: 6.1044e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 1.00000\n",
      "Epoch 40/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 6.7408e-04 - accuracy: 1.0000 - val_loss: 5.7050e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 1.00000\n",
      "Epoch 41/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 6.4673e-04 - accuracy: 1.0000 - val_loss: 5.3584e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 1.00000\n",
      "Epoch 42/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 5.3282e-04 - accuracy: 1.0000 - val_loss: 5.0749e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 1.00000\n",
      "Epoch 43/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 5.6394e-04 - accuracy: 1.0000 - val_loss: 4.7401e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 1.00000\n",
      "Epoch 44/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 4.9430e-04 - accuracy: 1.0000 - val_loss: 4.4833e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 1.00000\n",
      "Epoch 45/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 5.1610e-04 - accuracy: 1.0000 - val_loss: 4.2197e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 1.00000\n",
      "Epoch 46/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 4.3068e-04 - accuracy: 1.0000 - val_loss: 3.9897e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 1.00000\n",
      "Epoch 47/50\n",
      "71/71 [==============================] - ETA: 0s - loss: 4.2376e-04 - accuracy: 1.00 - 0s 2ms/step - loss: 4.2345e-04 - accuracy: 1.0000 - val_loss: 3.7817e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 1.00000\n",
      "Epoch 48/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 4.0664e-04 - accuracy: 1.0000 - val_loss: 3.5604e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 1.00000\n",
      "Epoch 49/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 4.1989e-04 - accuracy: 1.0000 - val_loss: 3.3739e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 1.00000\n",
      "Epoch 50/50\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 3.8708e-04 - accuracy: 1.0000 - val_loss: 3.1977e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 네트워크 구성\n",
    "# model.add(Dense(20, input_shape=(9, ), activation='relu'))\n",
    "model.add(Dense(5, input_dim=9, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) # 입력: 1,0,0,1,0,0,1,0,0 → 0 ~ 1 사이의 확률 3가지 출력, 총합은 1\n",
    "model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy', \n",
    "                                metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "mcp = ModelCheckpoint(filepath='./Book.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=1, restore_best_weights=True)\n",
    "\n",
    "hist = model.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=True,\n",
    "                 epochs=50, batch_size=1, callbacks=[mcp, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "# plt.figure(figsize=(6,4)) # ERROR\n",
    "fig.set_size_inches(10, 5)  # 챠트 크기 설정\n",
    "\n",
    "acc_ax = loss_ax.twinx()  # 오른쪽 y 출 설정\n",
    "\n",
    "# 왼쪽 y 축 설정\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5]) # 값을 반영하여 변경\n",
    "\n",
    "# 오른쪽 y 축 설정\n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train accuracy')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val accuracy')\n",
    "acc_ax.set_ylim([0.0, 1.0])\n",
    "\n",
    "# 축 레이블 설정\n",
    "loss_ax.set_xlabel('epoch' )  # 학습 횟수\n",
    "loss_ax.set_ylabel('loss')    # 왼쪽 y 축 레이블, 오차\n",
    "acc_ax.set_ylabel('accuracy') # 오른쪽 y 축 레이블,정확도\n",
    "\n",
    "loss_ax.legend(loc='upper left') # 왼쪽 y 축 오차 레이블 위치\n",
    "acc_ax.legend(loc='lower left')  # 오른쪽 y 축 정확도 레이블 위치\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손실값: 0.00031976556056179106 /정확도: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_val, y_val, batch_size=1, verbose=0)\n",
    "print('손실값:', test_loss, '/정확도:', (test_acc*100), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(path + '/Book.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터: (10, 9)\n",
      "데이터: [0.1 0.1 0.8 0.1 0.1 0.8 0.1 0.1 0.8]\n",
      "예측 결과 p.shape: (10, 3)\n"
     ]
    }
   ],
   "source": [
    "print('데이터:', x_test.shape) # 변수가 9개로 구성된 4건의 관측치(행)\n",
    "print('데이터:', x_test[0])    # 첫번째 데이터행\n",
    "\n",
    "p = model.predict(x_test)      # 테스트 데이터 4건 ★\n",
    "print('예측 결과 p.shape:', p.shape)     # (4, 3): 3: 폼종의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [8.363800e-09 3.019533e-06 9.999970e-01]\n",
      "예측값의 합: 1.000\n",
      "예측값: 0.00000% 0.00030% 99.99970%\n",
      "One-hot-encoding:  [0. 0. 1.]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print('예측값:', p[0])        # 첫번째 예측값 출력, 확률 0 ~ 1사이의 실수값\n",
    "print('예측값의 합: {0:0.3f}'.format(np.sum(p[0])))\n",
    "print('예측값: {0:.5f}% {1:.5f}% {2:.5f}%'.format(p[0,0]*100,p[0,1]*100,p[0,2]*100))\n",
    "print('One-hot-encoding: ', y_test[0])\n",
    "print(np.argmax(p[0]))      # 가장 큰값의 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.3638003e-09 3.0195331e-06 9.9999702e-01]\n",
      " [6.3898211e-04 9.9933904e-01 2.1877322e-05]\n",
      " [1.8186521e-09 2.3881637e-04 9.9976116e-01]\n",
      " [8.3638003e-09 3.0195331e-06 9.9999702e-01]\n",
      " [9.9948752e-01 5.1034632e-04 2.0892005e-06]\n",
      " [5.2403455e-05 4.0663499e-06 9.9994349e-01]\n",
      " [9.9931252e-01 6.7784439e-04 9.6646445e-06]\n",
      " [1.6177616e-08 9.9996078e-01 3.9203525e-05]\n",
      " [9.9975485e-01 1.5549110e-06 2.4362566e-04]\n",
      " [9.9999928e-01 3.1100041e-07 3.0497463e-07]]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
