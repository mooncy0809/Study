### 데이터(변수)의 종류

1. 질적 변수
   - 설문 조사: ① 매우 편리하다. ② 편리하다. ③ 보통이다. ④ 불편하다.
   - 혈액형, 지역등
   - 그룹화의 목적으로 사용되는 데이터

2. 양적 변수
   - 매출액, 금액, 연봉, 수익률등
   - 사칙연산이 가능한 데이터

3. 척도의 분류: 변수의 측정 단위
1) 명목척도(명의 척도)
    - 구분을 목적으로 사용되는 척도로서 숫자의 양적인 의미는 없으며,
      단지 자료가 지닌 속성을 상징적으로 구분한다.
      연산이 불가능한 변수
      예) 전철역명, 행정구역명, 성별(1-남자, 2-여자), 연령별(1-10대, 2-20대, 3-30대), 학력, 종교, 취미, 선수의 등번호, 지역명등     

2) 서열척도(순서 척도)
   - 관찰 대상이 순서적인 특성을 갖는것으로 연산은 불가능함,
     속성에 대한 각 수준 간의 간격이 동일한 경우.
     예) 좋아하는 차의 순서대로 숫자를 기록하시오.  커피(  ), 녹차(  ), 둥굴레차(  ), 아이스티(  ), 신용등급등

3) 등간척도(간격 척도)
    - 속성의 차이를 균일한 간격을 두고 분할하여 측정하는 척도, 리커트 5점, 7점 척도
     예) 스키장 중급 난이도는? ① 매우 어렵다. ② 어려운 편이다. ③ 보통이다. ④ 쉬운편이다.
         온도, 마이너스통장등 수치 데이터중에 -값이 있는 데이터

4) 비율척도(비례 척도)
   - 0점을 출발점으로하여 속성의 양적 차이를 나타내는 척도, 비율 계산, 사칙 연산이 가능하다.
     예) 매출액, 금액, 성적, 키, 무게, 인구수, 수량, 길이등

4. 수치형의 이산형 변수와 연속형 변수의 분리
1) 이산형 변수: 정수 형태로 카운트가 가능한 변수  예) 주문된 상품의 갯수
    예) 자신의 생활수준을 수치로 나타낸다면? 1 ~ 10 구간

2) 연속형 변수: 정수로 나누어 떨어지지 않는 변수  예) 길이나 무게, 시간등 무한대의 정밀도를 갖는 데인터
    - 연속형 변수를 정수형 변수로 변환하여 데이터하면 장점이 많음
      예) 키, 옷의 사이즈

### 1변량 데이터, 다변량 데이터
- 변량: 어떤 현상이 변화하는 양(크기), 일반적인 발생 데이터의 배열 예) 시간별 온도 변화등

1. 변량 데이터
   - 변수가 1개인 데이터


2. 다변량 데이터
   - 변수가 2개 이상인 데이터

### 모집단에서 표본 추출 시뮬레이션, 표본 통계량 성질, n-1, 대수의 법칙, 중심 극한 정리

1. 난수
- replace = False: 비복원 추출, 새로운 난수로 구성
- replace = True: 복원 추출, 같은 난수가 발생 될 수 있음, 같은 번호 추출 가능
- np.random.seed(1): 동일한 난수 발생, 1회만 사용 가능, 반복 실행시 효력이 상실됨.


2. 모수와 통계량 기호 
  구분        모수(모집단 대상)                     통계량(표본 대상)
  ---------   ----------------------------------    ----------------------------------
  의미        모집단의 특성을 나타내는 수치    표본의 특성을 나타내는 수치
  표기        그리스 문자                             영문 알파벳
  평균        μ(뮤, 모평균)                            (표본의 평균, xbar) 
  분산        σ²(시그마, 모분산)                     S²(표본의 분산) 
  표준편차  σ(모표준편차)                           S(표본의 표준편차) 
  대상 수    N(사례수)                                n(표본수)
- 표준 편차 구하는 순서: 평균 → 편차 → 분산 → 표준편차 → 표준화


3. 중심 극한 정리  
   - 표본집단의 크기가 커지면 그 표본평균은 모평균에 가까워진다.
   - 표본의 통계량을 연속 산출하면 정규 분포를 따른다(중심 극한 정리)

### 학습 방법별 기계 학습(Machine Learning)의 분류

1. 지도 학습(Supervised Learning)
   - 사람이 교사로서 각각의 입력(x)에 대해 레이블(y)을 달아놓은 데이터를 컴퓨터에 입력하면
     컴퓨터가 학습하는 방법
   - 사람이 직접 개입하므로 정확도가 높은 데이터를 사용할 수 있다는 장점이 있다. 
   - 대신에 사람이 직접 레이블을 달아야 하므로 인건비 문제가 있다.
   - 예: 알파고(아마추어 바둑기사의 대국을 통하여 학습)

1) 분류(Classification)
    - 레이블 y가 이산적(Discrete, 정수)인 경우 즉, y가 가질 수 있는 값이 [0,1,2 ..]와 같이 유한한 경우
      분류가 될 수 있다.
    - 일상에서 가장 접하기 쉬우며, 연구가 많이 되어있고, 기업들이 가장 관심을 가지는 문제 중 하나다.
    - 대표적인 기법: 로지스틱 회귀법, KNN, 서포트 벡터 머신 (SVM), 의사 결정 트리 등이 있다.
    - 예: 숫자 카드의 인식 문제, 독 버섯의 분류, 변호 성공 분류
    
2) 회귀(Regression)
    - 데이터 x를 이용해 y 를 예측 
    - 데이터가 1차원(1종류)인 회귀 분석 공식: f(x) = ax + b
    - 예: 투입된 광고비 대비 매출액 예측, 온도별 냉방 가전 제품 매출액 예출
    - 신경망의 주요 기반 기술: 회귀 --> 퍼셉트론 --> 딥러닝
    - 예) 시장 분석을 통한 매출액 분석(평판 분석, 트랜드 분석), 시장 분석을 통한 주가 예측
   

2. 비지도 학습(Unsupervised Learning)
   - 사람 없이 컴퓨터가 스스로 레이블 되어 있지 않은 데이터에 대해 학습하는 것. 
   - 즉 타겟 y없이 데이터 x만 이용해서 학습하는 것이다. 
   - 정답이 없는 문제를 푸는 것이므로 학습이 맞게 됐는지 확인할 길은 없지만,
     인터넷에 있는 거의 모든 데이터가 레이블이 없는 형태로 있으므로 앞으로
     기계학습이 나아갈 방향으로 설정되어 있기도 하다. 
   - 통계학의 군집화와 분포 추정 등의 분야와 밀접한 관련이 있다.
   - SKlearn package 주로 사용
   - 예: 썩은 과일의 분류 문제, 새로운 사용자 그룹의 발견, 새로운 품종을 발견하고 이름을 부여, SOM(자기 조직화 지도, 차원 축소)

1) 군집화(Clustering)
    - 나열된 데이터가 레이블이 없다고 해도 데이터간 거리에 따라 대충 두 세개의 군집으로 나눌 수 있다. 
      이렇게 데이터 x만 가지고 군집을 학습하는 것이 군집화이다.
2) 분포 추정(Underlying Probability Density Estimation)
    - 군집화에서 더 나아가서, 데이터들이 나열되 있을 때 어떤 확률 분포에서 나온 샘플들인지 추정하는 방법.


3. 반지도 학습 (Semisupervised learning)
   - 레이블이 있는 데이터와 없는 데이터 모두를 활용해서 학습하는 것인데, 
     대개의 경우는 다수의 레이블 없는 데이터를 약간의 레이블 있는 데이터로 보충해서
     학습하는 종류의 문제를 다룬다.


4. 강화학습(Reinforcement Learning)
   - 인간의 학습과 유사, 지도 학습이 이루어지고 나서 강화 학습을 진행(응용)
   - 강화학습은 현재의 상태(State)에서 어떤 행동(Action)을 취하는 것이 최적인지를 학습하는 것이다. 
   - 행동을 취할 때마다 외부 환경에서 보상(Reward)이 주어지는데,
     이러한 보상을 최대화 하는 방향으로 학습이 진행된다(예) 군견 훈련, 강아지 훈련).
   - 보상은 행동을 취한 즉시 주어지지 않을 수도 있다(지연된 보상).
   - 이 때문에 문제의 난이도가 지도나 비지도 학습에 비해 대폭 상승하며,
     시스템을 제대로 보상하는 것과 관련된 신뢰 할당 문제 라는 난제가 존재한다(어떻게 얼마의 크기로 보상을 할것인가?).
   - 강화 학습은 환경의 디지털화(학습 자료의 전처리)가 어려워 모델을 만들기 난해함.
   - 예: 게임 인공지능, 자율 주행, 알파고, 알파제로(스스로 바둑의 규칙을 알아내어 학습)등
   


### 알고리즘별 기계 학습(Machine Learning)의 분류

1. 회귀 기반
1) 회귀 분석 기반

2) 인공신경망(ANN:artificial neural network)
    - 인간의 신경망을 모방한 알고리즘: 퍼셉트론

   ① 퍼셉트론에 등장한 노드(신경)의 그룹화를 통하여 딥러닝 구현
       - 다층 퍼셉트론(MLP: Multilayer Perceptron): 회귀, 분류 문제 해결
       - 합성곱 신경망(CNN: Convolutional Neural Network): 이미지, 영상처리 분류
       - 순환 신경망(RNN: Recurrent neural network): 시계열 데이터 예측, 자연어처리, 음성 처리
       - 강화 학습(DQN: Deep Q-Network): 게임 인공 지능, 자율 주행 개발

   ② 딥러닝을 제품화
       - Tensorflow(Google): Python을 사용하나 많은 함수들이 제공되어 간결하게 최적화된 모델을 제작할 수 있음
         . 1.X 버전은 고유 문법이 매우 까다로워 사용이 불편하여 Pytorch로도 사용자가 많이 이동함.
         . 2.X 버전에서 까다로운 문법이 Keras를 이용하여 매우 간결하고, 단기간에 AI 모델을 제작 할 수 있는 환경을 제공 
       - PyTorch(Facebook): 순수 Python 코드 형태를 많이 사용, 구현이 자유로움


2. 확률 기반
1) 나이브 베이즈 분류(NBC: Naive Bayes Classifier): 스팸 메일 필터링등 텍스트 분류에 사용됨
2) 은닉 마르코프 모델(HMM: Hidden Markov Model)
    - 은닉 마르코프 모형에서 모수들이 알려졌음에도 불구하고 결과를 야기하는 상태들이
      근본적으로 은닉되어 있어 관찰할 수 없다.
    - 은닉된 상태들은 마르코프 과정을 통해 결과들로 도출된다. 
    - 은닉 마르코프 모형은 시간의 흐름에 따라 변화하는 시스템의 패턴을 인식하는 작업에 유용하다.
    - 음성 인식, 필기 인식(en:Handwriting recognition), 동작 인식(en:Gesture Recognition),
      품사 태깅(en:Part-of-speech tagging),
      악보에서 연주되는 부분을 찾는 작업, 부분 방전(en:Partial discharge), 생물정보학 분야에서 이용된다.


3. 기하 기반
1) k-평균 알고리즘(K-Means Clustering)
   - 주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을
     최소화하는 방식으로 동작한다.
   - 이 알고리즘은 자율 학습의 일종으로, 레이블이 달려 있지 않은 입력 데이터에 레이블을
     달아주는 역할을 수행한다.

2) k-최근접 이웃 알고리즘(K-NN: k-Nearest Neighbors)
   - 두 경우 모두 입력이 특징 공간 내 k개의 가장 가까운 훈련 데이터로 구성되어 있다.
   - 분류나 회귀에 사용되는 비모수 방식이다.

3) 서포트 벡터 머신(SVM: Support Vector Machine)
   - 기계 학습의 분야 중 하나로 패턴 인식, 자료 분석을 위한 지도 학습 모델이며,
     주로 분류와 회귀 분석을 위해 사용한다. 


4. 앙상블 기반(ensemble learning method)
   - 다양한 분류기의 예측 결과를 결합하여 예측값을 얻는다는 방법.
     단일 분류기에 비해 신뢰성이 높은것으로 알려져있다.



### 1차함수와 선형회귀
- 기울기와 절편을 알면 독립변수인 x를 대입하여 종속 변수인 y값을 예측 할 수 있음.

1. 1차함수 그래프
   - 기본 공식: y = ax + b
   - a: 기울기, y의 증가량 / x의 증가량
   - x: 데이터
   - b: y 절편(bias, 편향), x가 0일경우 y의 값
   - 온도에따른 에어콘 판매량<br>![image](https://user-images.githubusercontent.com/84116509/161496806-6e1023b7-36b2-42af-bf9c-ac321694a19b.png)
1) 1차원 벡터(단순 선형회귀 산출)에서의 기울기와 bias 산출(Xbar: X의 평균, Ybar: Y의 평균).
    - 데이터가 많아질수록 거의 계산이 어려움.
   Y = aX+b
   a= (∑XY - nXbarYbar) / (∑X²-nXbar²)
   b = Ybar - aXbar


2. 공부시간에 따른 성적의 예측
  . 공부시간 -> 시험 성적
          x             y
     원인 변수  결과 변수
     독립 변수  종속 변수
      정의역       치역
    -----------   ----------
    10 시간 ---> 70 점
    20 시간 ---> 74 점
    36 시간 ---> 76 점
    50 시간 ---> 82 점
    80 시간 ---> 92 점
  
3) 선형회귀 모델 제작은 a, b 변수의 값을 찾아내어 독립변수 x에 대한 최적의 y 값을
   산출하는 공식을 만드는 과정을 머신러닝이라고 함.
   - 선형회귀 모델: y = 3x + 76



### 편차와 오차
- 편차: 평균으로부터의 차이  예) 종각역 막차 11:30, 평균은 11:30, 열차 지연등으로 11:35분이면 편차가 생김.
- 오차: 예측한 값으로부터의 차이
  예) 종각역 막차 11:30일 것이라고 예측하고 있으나 열차 지연등으로 11:35분이면 5분의 오차가 생김.
- 현실 데이터는 오차가 존재 할 수 밖에 없음.

1. 평균 제곱 오차(MSE: Mean Square Error, 최소 제곱 오차), 오차 평가 알고리즘의 이용

1) 오차 = 예측 값(성적, 선형회귀 공식을 통해 산출한 값) - 실제 값(성적)  
   [공부한 시간]    2    4     6     8
   [성적(y)      ]   81   93   91    97
   [예측값(p)   ]   80   88   94   100
   [오차         ]    -1   -5    3      3   

 공부를 10시간 했을 경우의 성적을 예측하고 싶어 y = ax + b를 이용하여 성적을 예측하는
 모델을 제작하는 상황.
  
2) 오차의 제곱 합
    - 오차(편차)의 합은 0이 발생함으로 제곱을하여 오차를 산출함
    - 머신러닝: p: 예측값, y: 실제값, target
    - 수학: 편차의 합은 0, p: 성적 데이터, y: 평균

3) MSE(Mean Squared Error: 평균 제곱 오차)
   - 오차의 제곱의 합의 평균(분산공식)

4) RMSE(Root Mean Squared Error: 평균 제곱근 오차)(표준편차 공식)
   - √MSE == sqrt(MSE)
